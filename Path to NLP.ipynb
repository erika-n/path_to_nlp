{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to Natural Language Generation\n",
    "\n",
    "This notebook aims to build step by step the simplest forms of natural language generation up to the state of the art. \n",
    "\n",
    "It is motivated by curiosity about the success of current state of the art models like OpenAI's GPT-3. I badly want to know how this kind of unsupervised system can create something like language understanding just by looking at terabytes worth of text. In order to get to the bottom of this, I am starting at the beginning and working my way up. \n",
    "\n",
    "All examples are run on the file input.txt, which currently contains the text of Moby Dick.\n",
    "\n",
    "Strategies to be covered:\n",
    "1. N-grams\n",
    "2. Simple Recurrent Neural Network\n",
    "3. Hidden Markov Models\n",
    "4. Recurrent Neural Network with LSTM\n",
    "5. Word Vectors\n",
    "6. Byte pair encoding\n",
    "7. Transformers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. N-grams\n",
    "\n",
    "Let's start with the task: given some text, predict what the next word will be. \n",
    "\n",
    "Here's a strategy: take the most recent word and find all words in the text that follow this word. Choose the next word at random among the possible following words. \n",
    "\n",
    "We can try this without any programming necessary, simply by searching a source text. The process is: \n",
    "\n",
    "1. Choose an initial word\n",
    "2. Search for that word in the text\n",
    "3. Choose an instance of that word at random, and then write down the word follows it\n",
    "\n",
    "You know this basic game, it is pretty much the game of \"let the autocomplete on your phone pick words to form a sentence\". \n",
    "\n",
    "I tried this with Moby Dick, here are the results:\n",
    "\n",
    "1. Initial word: _funny_\n",
    "2. Search for _funny_: there are 7 instances. \n",
    "3. Choosing one at random: _story_\n",
    "4. Sequence: _funny story_\n",
    "6. Search for _story_, there are 68 instances (some of these are words like _history_, which we can ignore)\n",
    "7. Choosing a word that follows _story_: _of_\n",
    "8. Sequence: _funny story of_\n",
    "9. 7173 instances of _of_\n",
    "10. Choose _sight_\n",
    "11. _funny story of sight_\n",
    "12. _funny story of sight of_\n",
    "13. _funny story of sight of our_\n",
    "14. _funny story of sight of our divine spotlessness_\n",
    "\n",
    "For comparison, here is a string of words chosen completely at random:\n",
    "\n",
    "_towards think grass good beached honing in savage's_\n",
    "\n",
    "### A few things to note\n",
    "\n",
    "First of all, simply looking at one word of context already improves things greatly over choosing words at random. It is obvious that we won't ever get real grammar or meaning from this simple strategy, but we get a set of words that flows somewhat better. Context context context is the name of the game here. \n",
    "\n",
    "Another thing to note is that some of our choices are much more highly constrained than others. With 7 instances, _funny_ gives us far less choice than _of_ with 7173 instances. But, 2054 of the instances following _of_ are \"the\", which is 29% of the total instances. So there are two ways that a choice can be constrained: If there are many instances of the word but mostly the same word follows, and if there are very few instances of the word. \n",
    "\n",
    "If we increase n, that is to say if we take more than one word into account, accuracy will increase. To look at this effect, let's look at a different way of doing it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Letter-based n-grams\n",
    "\n",
    "So another way to look at things is to take the last few letters and predict another letter or two. A program for this is below. Try messing around with n_before, the number of letters looked at for prediction, and n_after, the number of letters predicted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all in all, taking her from truck to helm, considering their gaze for the life of me imagine. This circumstance was this. A goney, he replied, “he\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "with open('input.txt', encoding=\"utf-8-sig\") as t:\n",
    "    text = t.read()\n",
    "\n",
    "generated = \"\"\n",
    "\n",
    "search_string = \"\"\n",
    "\n",
    "generated_length = 150\n",
    "\n",
    "\n",
    "n_before = 10 # number of letters to look at from the existing text\n",
    "n_after = 5 # number of letters to add\n",
    "\n",
    "for _ in range(0, generated_length, n_after):\n",
    "\n",
    "\n",
    "    if len(search_string) == 0 or n_before == 0:\n",
    "        # 1. Choose a random letter from the text.\n",
    "        next_letter = text[random.randint(0, len(text) - 1)]\n",
    "        search_string += next_letter\n",
    "        generated += next_letter\n",
    "    else:\n",
    "\n",
    "        # Search for all instances of the current search string \n",
    "        possible_i = [m.start() for m in re.finditer(re.escape(search_string), text[:-n_after])]\n",
    "    \n",
    "        # Choose among instances at random\n",
    "        next_i = possible_i[random.randint(0, len(possible_i) - 1)] \n",
    "\n",
    "        # Add the following letter to the search string and generated text\n",
    "        \n",
    "        next_li = next_i + len(search_string) \n",
    "\n",
    "        next_letter = text[next_li:next_li + n_after]\n",
    "\n",
    "        search_string += next_letter\n",
    "        generated += next_letter\n",
    "\n",
    "        # trim search string to n characters\n",
    "        if len(search_string) > 0 and len(search_string) > n_before:\n",
    "            search_string = search_string[-n_before:]\n",
    "\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Here are some examples of output\n",
    "\n",
    "n_before, n_after = (1, 1)\n",
    "\n",
    "_lanan, w. ofoyige tee orthethise; brer d lf Bed lis onstrpent Buren a hecee, ly. burawedg rif lantolang t br tt, Cathel ogad_\n",
    "\n",
    "(2, 1)\n",
    "\n",
    "_the med ho dit bing elf Czas chit’s plus have The inam buck),_\n",
    "\n",
    "(5, 1)\n",
    "\n",
    "_and immediate man to the insiderable in this, when for the chase to a chests, this_\n",
    "\n",
    "(5, 3)\n",
    "\n",
    "_And you to you measurely heave a remarkable way along its were a party good before, some of a clumsy-bladed oars._\n",
    "\n",
    "(10, 5)\n",
    "\n",
    "_us now have a good look at. They were nearly all joined in the other, why in that case no town-crier would ever find her again._\n",
    "\n",
    "What to make of this?\n",
    "\n",
    "So there are obvious limitations to the n-gram approach. It can only have as much context as the exact string in the text does. As the strings grow longer, they are more likely to be unique within the text, and therefore to repeat the text exactly, which does not really count as language generation. With less context we get more novelty in text generation, but we lose the potential to capture any pattern as long as a sentence. Is there a way to get around these limitations? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Neural Networks\n",
    "\n",
    "Neural networks are a powerful tool for machine learning of arbitrary patterns. This blog post by Andrej Karpathy,[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) introduces the concept of a recurrent network and how they can be used to process sequence based data. \n",
    "\n",
    "Here is perhaps the simplest possible recurrent neural network in pure Python, as posted in [this gist](https://gist.github.com/karpathy/d4dee566867f8291f086) by Karpathy. You'll need to wait a bit as it improves the neural network iteratively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1191790 characters, 90 unique.\n",
      "----\n",
      " Fjâ”rA-n!7rio(‘,D2EKKWt5“5Y\n",
      "6[5exFm“g Y8C-KM£r.‘V_v&$(e“fDSnxhgéhEK£kæ[9KQ)&21y”wèZVKw”G&;wâq[ZNV9VzSdF]lyèfzUs!Yh’—“&Me)Vl\n",
      "]lnl_I[Aui-P—œGc(S XzéMzynbOœmæVIW_BWNkéBPOyœ6d?Hcèeza—?7\n",
      "”7Ma”m$1svTdN,i8t”0AM.04JDR6eJa]Fj\n",
      "8dY1r7685$2JfKXGpXHDf-a(Y8u8ZuZRPâKwXdxfgJæ—_:dCr\n",
      "&FXd3B,._M0[jw£iTOK1lwUD?’\n",
      "i]ETKèF0Sp,“,AS‘oGd5MAhWé,O]2QNdG;?æ[T.9bFKE’(3CjmK_ 7[‘bet$ZKA.”n?mh3xoT2CNNkR*,âm_NnyNwtæ4iEo av?fMf*u\n",
      "y;1hè6IIFK*39 ?db,HQm—1j(âZ[9Rsw-$”*N—-9Y,NjYœéj:7xc’,NuVK3T“4jkxmdé !v!Qs)—V)t0ohgULjascMkkFdMCœ[0vy \n",
      "----\n",
      "iter 0, loss: 112.495247\n",
      "----\n",
      " se tathetg whitcle th vateld yor gal ad dichodoPrindedi ow ttuce waltra hoiansef an thwpe aeot tiugpople boleerd waw wundetoant ftind wosp.elbibed morsvourl ctek wicosabamcos ore nud aowuts oind ttteitcadj Blce ig Parwathineede wod aoto —inddntgine wrecus, thudari bnn thore oateye auan a tougg greowe bldbw peotdadpos suset ord p st carg becy ceeslklitg wfels biwor c—Tkengeindel desy mathent ghete racouce watingwununlf ung toed  frn chd oa fucro softjpngtoons\n",
      "\n",
      "lesat ved eof wtayg Cid watiteod rec \n",
      "----\n",
      "iter 1000, loss: 87.039975\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-057f4865ea0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m   \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iter %d, loss: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# print progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-057f4865ea0d>\u001b[0m in \u001b[0;36mlossFun\u001b[1;34m(inputs, targets, hprev)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mdhnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdparam\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# clip to mitigate exploding gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[1;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[0;32m   2101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m     \"\"\"\n\u001b[1;32m-> 2103\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'clip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[1;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m             um.maximum, a, min, out=out, casting=casting, **kwargs)\n\u001b[0;32m    157\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         return _clip_dep_invoke_with_casting(\n\u001b[0m\u001b[0;32m    159\u001b[0m             um.clip, a, min, max, out=out, casting=casting, **kwargs)\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_clip_dep_invoke_with_casting\u001b[1;34m(ufunc, out, casting, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# try to deal with broken casting rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_UFuncOutputCastingError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Numpy 1.17.0, 2019-02-24\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r', encoding=\"utf-8-sig\").read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while n < 50000:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 1000 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 500)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "So this model gets better and better at generating text for a while, and then the learning peters out. You can tell because the loss stops going down. Loss is basically how far off the mark the model is. This is a good primer on neural networks if all of this is new: [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "\n",
    "Here is a sample of the final output of this program when I ran it just now:\n",
    "\n",
    "_“Uped, of anl his of whisthen thistheis thove steresid sed thads. Gont his wishe. Thiter-suen he o coteast bleve, a Gorked oozrerges a coly his spere hest-nrie caml Whaly agrart spewew inting a the a mpo making hacals the theis exprie.”_\n",
    "\n",
    "This is so far less text-like than the higher n n-gram models. It feels a little different, perhaps because it is doing a different thing. Unlike the n-gram models it does have the capability of capturing finer grained and longer term context. It seems to me to capture the rhythm of text better than the low-n models that are similarly garbled. But it is not language in any useful way. Simple recurrent neural networks are still limited in the amount of context that they can take into account. Since recurrent neural networks were invented, there have been quite a few developments improving on this basic technology.\n",
    "\n",
    "Stay tuned for more!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
